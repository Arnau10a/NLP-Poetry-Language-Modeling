{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-hHGbc_k6Qb"
      },
      "source": [
        "# NLP Assignment: Language Modelling with Poetry (deadline **24.5.2024**)\n",
        "\n",
        "This assignment will check your basic NLP understanding through the fundamental NLP task of **language modelling**.\n",
        "\n",
        "You will reiterate on the task with techniques ranging from simple n-gram counting to embeddings and deep learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g92L3xquSd-"
      },
      "source": [
        "You will work with the same short poetry texts that should be very familiar to you by now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjOGsU4vg6nH"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/GustikS/smu-nlp/master/robert_frost.txt\n",
        "!wget -nc https://raw.githubusercontent.com/GustikS/smu-nlp/master/edgar_allan_poe.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZ9k9_fGTgH"
      },
      "source": [
        "Unless stated otherwise (Ex. 4,5,6), work just with the Robert Frost file for simplicity (Ex. 1,2,3,7,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz2TlSVZwkiF",
        "outputId": "c605df18-3f5d-43f1-cf48-0af83f8ee183"
      },
      "outputs": [],
      "source": [
        "!head  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8EBH2lVBEWe"
      },
      "source": [
        "#### **Excercise 1**: Markov Language Model (**2 points**)\n",
        "\n",
        "1. Create a 1st order Markov (bi-gram) language model\n",
        "  - work with matrix representation of the model\n",
        "    - i.e. not dictionaries as we did in the tutorial!\n",
        "    - hence you'll need to assign indices to the words, too\n",
        "      - include an extra \\<UNK\\> token for unseen words\n",
        "  - correctly handle beginnings and ends of sentences\n",
        "    - sentence = line (skip empty lines)\n",
        "  - lower case and properly tokenize your sentences\n",
        "    - but skip all other text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_markov_model(filename):\n",
        "    \"\"\"\n",
        "    Creates a 1st order Markov model from a text file using matrix representation.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - word_to_index: Dictionary mapping words to unique integer indices.\n",
        "            - transition_matrix: A 2D numpy array representing transition probabilities.\n",
        "            - unk_index: The index assigned to the <UNK> token.\n",
        "    \"\"\"\n",
        "    # Build word dictionary and assign unique indices\n",
        "    word_to_index = {\"<UNK>\": 0, \"<s>\": 1, \"</s>\": 2}  # Include <s> and </s>\n",
        "    index_to_word = [\"<UNK>\", \"<s>\", \"</s>\"]  # Reverse mapping\n",
        "    word_count = len(word_to_index)\n",
        "\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            tokens = line.rstrip().lower().split()\n",
        "            if not tokens:\n",
        "                continue\n",
        "            for word in tokens:\n",
        "                if word not in word_to_index:\n",
        "                    word_to_index[word] = word_count\n",
        "                    index_to_word.append(word)\n",
        "                    word_count += 1\n",
        "\n",
        "    # Initialize transition matrix with zeros\n",
        "    vocab_size = len(word_to_index)\n",
        "    transition_matrix = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "    # Process text to build transition counts\n",
        "    unk_index = word_to_index[\"<UNK>\"]\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            tokens = line.rstrip().lower().split()\n",
        "            if not tokens:\n",
        "                continue\n",
        "            tokens = ['<s>'] + tokens + ['</s>']  # Add start and end tokens\n",
        "\n",
        "            # Use loop variable for iteration\n",
        "            for i in range(len(tokens) - 1):\n",
        "                prev_word_index = word_to_index.get(tokens[i], unk_index)\n",
        "                next_word_index = word_to_index.get(tokens[i + 1], unk_index)\n",
        "                transition_matrix[prev_word_index, next_word_index] += 1\n",
        "\n",
        "    # Normalize transition counts to probabilities\n",
        "    for i in range(vocab_size):\n",
        "        total_count = np.sum(transition_matrix[i])\n",
        "        if total_count > 0:\n",
        "            transition_matrix[i] /= total_count\n",
        "\n",
        "    return word_to_index, transition_matrix, unk_index\n",
        "\n",
        "# Example usage\n",
        "word_to_index, transition_matrix, unk_index = create_markov_model(\"robert_frost.txt\")\n",
        "\n",
        "word1 = \"the\"\n",
        "word2 = \"young\"\n",
        "print(f\"Transition probability from '{word1}' to '{word2}': {transition_matrix[word_to_index[word1], word_to_index.get(word2, unk_index)]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUD5eCd0PQ3w"
      },
      "source": [
        "#### **Excercise 2**: Probability + Smoothing (**1 points**)\n",
        "1. write a function to obtain probability of a given sentence with your model\n",
        "    - include the beginning and end mark of the sentence as well\n",
        "    - test some sentences and assure the probability makes sense\n",
        "2. incorporate the add-1 (Laplace) smoothing to account for unseen bi-grams\n",
        "    - you should have your \\<UNK\\> for unseen unigrams already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentence_probability(sentence, word_to_index, transition_matrix, unk_index):\n",
        "    \"\"\"\n",
        "    Computes the probability of a sentence using the Markov model.\n",
        "\n",
        "    Args:\n",
        "        sentence: The sentence for which to compute the probability.\n",
        "        word_to_index: Dictionary mapping words to unique integer indices.\n",
        "        transition_matrix: A 2D numpy array representing transition probabilities.\n",
        "        unk_index: The index assigned to the <UNK> token.\n",
        "\n",
        "    Returns:\n",
        "        The probability of the sentence.\n",
        "    \"\"\"\n",
        "    tokens = sentence.lower().split()\n",
        "    prob = 1.0\n",
        "\n",
        "    \n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        prev_word_index = word_to_index.get(tokens[i], unk_index)\n",
        "        next_word_index = word_to_index.get(tokens[i + 1], unk_index)\n",
        "        prob *= transition_matrix[prev_word_index, next_word_index]\n",
        "\n",
        "    return prob\n",
        "\n",
        "def laplace_smoothing(transition_matrix):\n",
        "    \"\"\"\n",
        "    Applies Laplace smoothing to the transition matrix.\n",
        "\n",
        "    Args:\n",
        "        transition_matrix: A 2D numpy array representing transition counts.\n",
        "\n",
        "    Returns:\n",
        "        A 2D numpy array representing smoothed transition probabilities.\n",
        "    \"\"\"\n",
        "    vocab_size = transition_matrix.shape[0]\n",
        "    smoothed_matrix = transition_matrix + 1  # Add 1 to each count\n",
        "\n",
        "    # Normalize with added vocabulary size\n",
        "    for i in range(vocab_size):\n",
        "        total_count = np.sum(smoothed_matrix[i])\n",
        "        smoothed_matrix[i] /= total_count\n",
        "\n",
        "    return smoothed_matrix\n",
        "\n",
        "# Example usage\n",
        "sentence = \"<s> the young folk held some hope out to each other. </s>\"\n",
        "print(f\"Sentence probability: {sentence_probability(sentence, word_to_index, transition_matrix, unk_index)}\")\n",
        "\n",
        "transition_matrix_smoothed = laplace_smoothing(transition_matrix)\n",
        "print(f\"Sentence probability with Laplace smoothing: {sentence_probability(sentence, word_to_index, transition_matrix_smoothed, unk_index)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ_eRLBU_sRI"
      },
      "source": [
        "#### **Excercise 3**: Perplexity (**1 points**)\n",
        "1. write a function fo calculate perplexity of your smoothed model on a given sentence\n",
        "  - including its beginning and ending\n",
        "2. find the sentence(s) from the corpus with minimum and maximum perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_perplexity(sentence, word_to_index, transition_matrix, unk_index):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of a given sentence using the Markov model.\n",
        "\n",
        "    Args:\n",
        "        sentence: The input sentence as a string.\n",
        "        word_to_index: Dictionary mapping words to unique integer indices.\n",
        "        transition_matrix: A 2D numpy array representing transition probabilities.\n",
        "        unk_index: The index assigned to the <UNK> token.\n",
        "\n",
        "    Returns:\n",
        "        The perplexity of the sentence.\n",
        "    \"\"\"\n",
        "    tokens = sentence.lower().split()\n",
        "    length = len(tokens)\n",
        "    ps = []\n",
        "\n",
        "    for i in range(length - 1):\n",
        "        ps.append(transition_matrix[word_to_index.get(tokens[i], unk_index), word_to_index.get(tokens[i + 1], unk_index)])\n",
        "\n",
        "    val = 1\n",
        "    for p in ps:\n",
        "        val *= 1/p\n",
        "    val = val**(1/length)\n",
        "    return val\n",
        "\n",
        "def find_min_max_perplexity_sentences(filename, word_to_index, transition_matrix, unk_index):\n",
        "    \"\"\"\n",
        "    Finds the sentences with the minimum and maximum perplexity from the corpus.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the text file.\n",
        "        word_to_index: Dictionary mapping words to unique integer indices.\n",
        "        transition_matrix: A 2D numpy array representing transition probabilities.\n",
        "        unk_index: The index assigned to the <UNK> token.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - Sentence with minimum perplexity\n",
        "            - Sentence with maximum perplexity\n",
        "            - Minimum perplexity value\n",
        "            - Maximum perplexity value\n",
        "    \"\"\"\n",
        "    min_perplexity = float('inf')\n",
        "    max_perplexity = float('-inf')\n",
        "    min_sentence = \"\"\n",
        "    max_sentence = \"\"\n",
        "\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            tokens = line.rstrip().lower().split()\n",
        "            if not tokens:\n",
        "                continue\n",
        "            sentence = ' '.join(tokens)\n",
        "            perplexity = calculate_perplexity(sentence, word_to_index, transition_matrix, unk_index)\n",
        "            if perplexity < min_perplexity:\n",
        "                min_perplexity = perplexity\n",
        "                min_sentence = sentence\n",
        "            if perplexity > max_perplexity:\n",
        "                max_perplexity = perplexity\n",
        "                max_sentence = sentence\n",
        "\n",
        "    return min_sentence, max_sentence, min_perplexity, max_perplexity\n",
        "\n",
        "# Calculate the perplexity of a sentence\n",
        "sentence = \"the young folk held some hope out to each other. </s>\"\n",
        "perplexity = calculate_perplexity(sentence, word_to_index, transition_matrix, unk_index)\n",
        "print(f\"Perplexity of sentence '{sentence}': {perplexity}\")\n",
        "\n",
        "# Find the sentences with minimum and maximum perplexity from the corpus\n",
        "min_sentence, max_sentence, min_perplexity, max_perplexity = find_min_max_perplexity_sentences(\"robert_frost.txt\", word_to_index, transition_matrix, unk_index)\n",
        "print(f\"Sentence with minimum perplexity: '{min_sentence}' (Perplexity: {min_perplexity})\")\n",
        "print(f\"Sentence with maximum perplexity: '{max_sentence}' (Perplexity: {max_perplexity})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ukRce0_myA"
      },
      "source": [
        "#### **Excercise 4:**  Markov classifier (**4 points**)\n",
        "Implement your own probabilistic classifier based on your bi-gram language model. That is:\n",
        "  1. given some classes of sentences, train a separate language model for each class respectively\n",
        "  2. then classify a given sentence (=sample) based on maximum a-posteriori probability (MAP)\n",
        "    - i.e. don't forget about the class priors, too!\n",
        "    - use log probabilities!\n",
        "    - make sure your smoothing treats all the classes equally!\n",
        "      - ...think about what happens to the UNK token\n",
        "  3. evaluate on a task of recognizing sentences from the 2 different poets (Frost vs. Poe)\n",
        "    - split the sentences (=samples) from each poet into train-test in advance!\n",
        "      - train-test split 70:30\n",
        "        - do not shuffle sentences\n",
        "      - skip empty lines (of course)\n",
        " \t- report all accuracy values + a confusion matrix\n",
        "\n",
        "*Note that this is very similar to our previous classification with Naive Bayes, but with bi-grams instead of unigrams.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "def classify_sentence(sentence, frost_model, poe_model, frost_prior, poe_prior):\n",
        "    frost_log_prob = sentence_probability(sentence, *frost_model) + np.log(frost_prior)\n",
        "    poe_log_prob = sentence_probability(sentence, *poe_model) + np.log(poe_prior)\n",
        "\n",
        "    return \"Frost\" if frost_log_prob > poe_log_prob else \"Poe\"\n",
        "\n",
        "def load_data(filename):\n",
        "    with open(filename) as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "def split_data(data, train_ratio=0.7):\n",
        "    train_size = int(len(data) * train_ratio)\n",
        "    return data[:train_size], data[train_size:]\n",
        "\n",
        "# Load and split data\n",
        "frost_data = load_data(\"robert_frost.txt\")\n",
        "poe_data = load_data(\"edgar_allan_poe.txt\")\n",
        "\n",
        "frost_train, frost_test = split_data(frost_data)\n",
        "poe_train, poe_test = split_data(poe_data)\n",
        "\n",
        "# Save training data for model creation\n",
        "with open(\"frost_train.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(frost_train))\n",
        "with open(\"poe_train.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(poe_train))\n",
        "\n",
        "# Train models\n",
        "frost_model = create_markov_model(\"frost_train.txt\")\n",
        "poe_model = create_markov_model(\"poe_train.txt\")\n",
        "\n",
        "# Apply Laplace smoothing\n",
        "frost_model = (frost_model[0], laplace_smoothing(frost_model[1]), frost_model[2])\n",
        "poe_model = (poe_model[0], laplace_smoothing(poe_model[1]), poe_model[2])\n",
        "\n",
        "# Class priors\n",
        "frost_prior = len(frost_train) / (len(frost_train) + len(poe_train))\n",
        "poe_prior = len(poe_train) / (len(frost_train) + len(poe_train))\n",
        "\n",
        "# Classify test sentences\n",
        "y_true = [\"Frost\"] * len(frost_test) + [\"Poe\"] * len(poe_test)\n",
        "y_pred = []\n",
        "\n",
        "for sentence in frost_test:\n",
        "    y_pred.append(classify_sentence(sentence, frost_model, poe_model, frost_prior, poe_prior))\n",
        "\n",
        "for sentence in poe_test:\n",
        "    y_pred.append(classify_sentence(sentence, frost_model, poe_model, frost_prior, poe_prior))\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "conf_matrix = confusion_matrix(y_true, y_pred, labels=[\"Frost\", \"Poe\"])\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqOHBzHurTsM"
      },
      "source": [
        "#### **Excercise 5**: PPMI word-word cooccurence (**2 points**)\n",
        "1. Create a word-word co-occurence matrix from all the text of both the poets altogether\n",
        "  - use a sliding window of size 5 (2 left + 2 right context words)\n",
        "    - remember that you can reuse existing solutions...\n",
        "2. Post-process the matrix with PPMI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def co_occurrence(texts, context_size):\n",
        "    window_size = (context_size - 1) // 2\n",
        "    cooccurrence = {}\n",
        "    vocab = set()\n",
        "    \n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text)\n",
        "        for token in tokens:\n",
        "            vocab.add(token)\n",
        "\n",
        "        # Sliding window\n",
        "        for i in range(window_size, len(tokens) - window_size):\n",
        "            token = tokens[i]\n",
        "            context = tokens[(i - window_size):i] + tokens[(i + 1):(i + 1 + window_size)]\n",
        "            for t in context:\n",
        "                key = tuple(sorted([t, token]))\n",
        "                cooccurrence[key] = cooccurrence.get(key, 0) + 1\n",
        "\n",
        "    # Formulate the dictionary into a dataframe\n",
        "    vocab = sorted(vocab)\n",
        "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.float64), index=vocab, columns=vocab)\n",
        "    for key, value in cooccurrence.items():\n",
        "        df.at[key[0], key[1]] = value\n",
        "        df.at[key[1], key[0]] = value\n",
        "    return df\n",
        "\n",
        "def ppmi(df, positive=True):\n",
        "    col_totals = df.sum(axis=0)\n",
        "    row_totals = df.sum(axis=1)\n",
        "    total = col_totals.sum()\n",
        "    expected = np.outer(row_totals, col_totals) / total\n",
        "    df = df / expected\n",
        "    df = np.log(df)\n",
        "\n",
        "    df[np.isinf(df)] = 0.0  # log(0) = 0  ; or silence distracting warnings about log(0) with np.errstate(divide='ignore'):\n",
        "    # ppmi\n",
        "    if positive:\n",
        "        df[df < 0] = 0.0\n",
        "    return df\n",
        "\n",
        "# Reading text files\n",
        "with open('edgar_allan_poe.txt', 'r') as file:\n",
        "    poe_text = file.read()\n",
        "\n",
        "with open('robert_frost.txt', 'r') as file:\n",
        "    frost_text = file.read()\n",
        "\n",
        "# Combine the texts\n",
        "texts = [poe_text, frost_text]\n",
        "context_size = 3\n",
        "\n",
        "# Generate the co-occurrence matrix\n",
        "co_occurrence_matrix = co_occurrence(texts, context_size)\n",
        "\n",
        "# Compute the PPMI matrix using the provided function\n",
        "ppmi_matrix = ppmi(co_occurrence_matrix)\n",
        "\n",
        "# Display the PPMI matrix\n",
        "print(ppmi_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUWr5HsIry66"
      },
      "source": [
        "#### **Excercise 6**: Word embeddings (**1 points**)\n",
        "1. Extract 8-dimensional word embeddings from your PPMI matrix\n",
        "  - using the truncated SVD matrix decomposition\n",
        "2. Plot them in 2D with word labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "svd = TruncatedSVD(n_components=8)\n",
        "word_embeddings = svd.fit_transform(ppmi_matrix)\n",
        "\n",
        "# Further reduce the dimensionality to 2 dimensions for plotting\n",
        "svd_2d = TruncatedSVD(n_components=2)\n",
        "word_embeddings_2d = svd_2d.fit_transform(word_embeddings)\n",
        "\n",
        "# Plotting the 2D embeddings with word labels\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(word_embeddings_2d[:, 0], word_embeddings_2d[:, 1], marker='o')\n",
        "\n",
        "for i, word in enumerate(ppmi_matrix.index):\n",
        "    plt.annotate(word, (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]), fontsize=12)\n",
        "\n",
        "plt.title('2D Word Embeddings from PPMI Matrix')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OovsDDLftlXo"
      },
      "source": [
        "#### **Excercise 7:** LSTM Language Model (**3 points**)\n",
        "1. Formulate a proper dataset for language modelling on longer sequences from the text\n",
        "  - beyond the n-gram scope, use 10 consecutive words\n",
        "2. Create a suitable LSTM-based language model\n",
        "3. Initialize the embedding layer of your model with your \"pretrained\" SVD embeddings\n",
        "4. Train the model in a suitable fashion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Read and combine texts\n",
        "with open('edgar_allan_poe.txt', 'r') as file:\n",
        "    poe_text = file.read()\n",
        "\n",
        "with open('robert_frost.txt', 'r') as file:\n",
        "    frost_text = file.read()\n",
        "\n",
        "texts = poe_text + ' ' + frost_text\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(texts.lower())\n",
        "\n",
        "# Create sequences of 10 words\n",
        "sequence_length = 10\n",
        "sequences = []\n",
        "\n",
        "for i in range(sequence_length, len(tokens)):\n",
        "    seq = tokens[i-sequence_length:i+1]\n",
        "    sequences.append(seq)\n",
        "\n",
        "# Prepare tokenizer (mapping from word to index)\n",
        "vocab = list(set(tokens))\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "sequences = [[word2idx[word] for word in seq] for seq in sequences]\n",
        "\n",
        "# Split sequences into input (X) and output (y)\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoader instances\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embeddings=None):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if embeddings is not None:\n",
        "            self.embedding.weight = nn.Parameter(torch.tensor(embeddings, dtype=torch.float32))\n",
        "            self.embedding.weight.requires_grad = False  # Optionally freeze the embedding layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, dropout=0.2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 8\n",
        "hidden_dim = 100\n",
        "output_dim = vocab_size\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZugaJlYWuQ4"
      },
      "source": [
        " #### **Excercise 8**: Sampling (**1 point**)\n",
        "1. Sample some text from your models w.r.t. to the output (next) word probability distribution\n",
        "  - for both bigram and LSTM models - which is better?\n",
        "\n",
        "2. Paste your best \"poem\" here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYJakWdKtP4M"
      },
      "source": [
        "# **Notes**\n",
        "\n",
        "If this seems like a lot of work, reiterate through the tutorials (which is the purpose, anyway) - we did most of this already!\n",
        "  - i.e., I don't expect you to be overly creative, you can find all the pieces in the tutorial notebooks\n",
        "  - but feel free to write a more efficient/clean/suitable code from scratch\n",
        "  - only the libraries we used in the tutorials are allowed\n",
        "  - teamwork is forbidden!\n",
        "  - chatGPT is allowed\n",
        "\n",
        "Before submitting:\n",
        "  - make sure your code is runnable (in Colab)!\n",
        "  - comment your code at least a bit, make it readable!\n",
        "  - add printouts after every excercise\n",
        "  - submit just the notebook as a single file (filename = username)\n",
        "\n",
        "\n",
        "I will only evaluate *correctness* of the solutions, not quality of the models\n",
        "  - i.e. no need to spent too much time with text preprocessing or hyperparameter tuning (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgZq8YiueTqH"
      },
      "source": [
        "# **Bonus points**\n",
        "You can recover some of the potentially lost points here, but you can't reach more than 15 points in total from this assignment\n",
        "  - *explicitly mark the bonus points you consider as fullfilled!*\n",
        "<br/><br/>\n",
        "\n",
        "1. (1p) Use a 2nd order Markov model with a sparse tensor instead\n",
        "  - i.e., do Laplace smoothing without actually modifying the model (tensor)\n",
        "\n",
        "2. (1p) In your LSTM model, do not start every sample sequence with an empty hidden state (zeros), but pass the last hidden state from the preceding sample sequence\n",
        "  - i.e. the sequence ending just before the start of the current sequence\n",
        "  - but do not propagate gradient into the previous sample (detach the state value from the backprop graph)\n",
        "\n",
        "3. (1p) Improve your text generation by sampling from the top-p next words only\n",
        "\n",
        "4. (2p) Use a different (larger) textual corpus of poems\n",
        "  - e.g. Corpus of Czech Verse, collected at the Institute of Czech Literature at Czech Academy of Sciences\n",
        "    - available at https://github.com/versotym/corpusCzechVerse\n",
        "    - just be careful about czech character encoding\n",
        "\n",
        "5. (10p) An alternative task - create a \"whisperer\" (bot player) for the popular game [\"Kryci jmena\"](https://krycijmena.cz/) based on word embeddings\n",
        "  - i.e. automatically search for words that would cover (be related/close to) subsets of the \"positive\" words, while trying to avoid the \"negative\" words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac1PCrEHmHgF",
        "outputId": "3d2d6569-a03a-4fff-ae3b-24b2128354d0"
      },
      "outputs": [],
      "source": [
        "# Corpus of Czech Verse\n",
        "!git clone https://github.com/versotym/corpusCzechVerse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ5n5X2UMGLh",
        "outputId": "2208eb28-7c1f-44ea-a5bf-a295f6d691dd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from os import listdir\n",
        "path = 'corpusCzechVerse/ccv/'\n",
        "\n",
        "books = []\n",
        "files = [f for f in listdir(path)]\n",
        "for i, f in enumerate(sorted(files)):\n",
        "    print(f)\n",
        "    book = open(path+f)\n",
        "    books.append(json.load(book))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dkic4r2IPnN8",
        "outputId": "78eff66e-e475-47f6-8368-5146ec24e040"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "book = books[i]\n",
        "\n",
        "for poem in book:\n",
        "    author = poem['p_author']['name']\n",
        "    title = poem['biblio']['p_title']\n",
        "    print('------' + title + '------\\n')\n",
        "    body = poem['body']\n",
        "    for text in body:\n",
        "        for line in text:\n",
        "            print(line['text'])\n",
        "        print('')\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kgZq8YiueTqH"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
